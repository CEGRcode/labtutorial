{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction # This website contains installation guides to setup primary software packages and development infrastructure for Mac Workstations. Maintained and Curated by PughLab . Infrastructure Details # The lab takes advantage of GALAXY to create data analysis visualization pipelines for large-scale data processing on the ACI-ICS clusters and also leverages XSEDE resources for compute requirements. For day to day bioinformatics, install start using Script Manager. ICS-ACI Resource URL Account Setup User's Guide Make sure you are added to the PughLab ACI allocations and relevant group space once you get an ACI account. Software Stacks # Below are some the Web development stacks being used in the lab. Project Abbreviation Stack Yeast Epigenome Project YEP MERN Protein Capture Reagent Program Validation PCRP MERN Platform for Epigenetic and Genomic Research PEGR MySql , Grails , Groovy MERN Stack is made up of MongoDB , Express , React , NodeJS For more project repositories, go to PughLab's GitHub Development Tools # Install the missing package manager for macOS : Homebrew Command line tools using Xcode, paste this command in a terminal : xcode-select --install For Python3 Python2.x development, download install Anaconda Install Java8.x and Java JDK before you download the latest release of ScriptManager Tutorials Reading Material # Understanding Shell scripts, Markdown, Git and start using ACI clusters. PSU Biostars Bootcamp #day2 Beginner Web Development tutorials MDN Web Docs Web technologies at a glance. HTML CSS Javascript Understanding React Why did they build React ? Official React docs Creating REST API with Node.js video tutorial Creating React Applications video tutorial Contributors # Name Lab Position Prashant Kumar Kuntala Computational Scientist Hedgie Jo Undergraduate Researcher Pierce Chaffin Undergraduate Researcher","title":"Home"},{"location":"#introduction","text":"This website contains installation guides to setup primary software packages and development infrastructure for Mac Workstations. Maintained and Curated by PughLab .","title":"Introduction"},{"location":"#infrastructure-details","text":"The lab takes advantage of GALAXY to create data analysis visualization pipelines for large-scale data processing on the ACI-ICS clusters and also leverages XSEDE resources for compute requirements. For day to day bioinformatics, install start using Script Manager. ICS-ACI Resource URL Account Setup User's Guide Make sure you are added to the PughLab ACI allocations and relevant group space once you get an ACI account.","title":"Infrastructure Details"},{"location":"#software-stacks","text":"Below are some the Web development stacks being used in the lab. Project Abbreviation Stack Yeast Epigenome Project YEP MERN Protein Capture Reagent Program Validation PCRP MERN Platform for Epigenetic and Genomic Research PEGR MySql , Grails , Groovy MERN Stack is made up of MongoDB , Express , React , NodeJS For more project repositories, go to PughLab's GitHub","title":"Software Stacks"},{"location":"#development-tools","text":"Install the missing package manager for macOS : Homebrew Command line tools using Xcode, paste this command in a terminal : xcode-select --install For Python3 Python2.x development, download install Anaconda Install Java8.x and Java JDK before you download the latest release of ScriptManager","title":"Development Tools"},{"location":"#tutorials-reading-material","text":"Understanding Shell scripts, Markdown, Git and start using ACI clusters. PSU Biostars Bootcamp #day2 Beginner Web Development tutorials MDN Web Docs Web technologies at a glance. HTML CSS Javascript Understanding React Why did they build React ? Official React docs Creating REST API with Node.js video tutorial Creating React Applications video tutorial","title":"Tutorials &amp; Reading Material"},{"location":"#contributors","text":"Name Lab Position Prashant Kumar Kuntala Computational Scientist Hedgie Jo Undergraduate Researcher Pierce Chaffin Undergraduate Researcher","title":"Contributors"},{"location":"Backend Deployment/","text":"Backend Deployment # Instructions to deploy web apps built using NodeJS on PSU's VM hosting servers. Deploying the application # Below instructions are for a server running CentOS . However, app related configuration is still valid for any linux distribution. Install dependencies on the Server Install node (stable version) for CentOS , here is a tutorial . Install mongodb on the CentOS server. Install pm2 a production process manager for Node.js applications with a built-in load balancer. Server Specific Configuration Create a .env file. Add and edit below fields to appropriate values : DB_URL= DB_NAME= DATASETS_PATH= PUBLIC_ENDPOINT= IMAGE_URL= NODE_PORT= For example, a .env for a server example.vmhost.psu.edu by a user bob looks like below: DB_URL= localhost DB_NAME= testDB DATASETS_PATH= /home/bob/imageAssets PUBLIC_ENDPOINT= https://example.vmhost.psu.edu:8081/samples/ IMAGE_URL= https://example.vmhost.psu.edu:8081/images/ NODE_PORT=8081 Copy your files, images and other assets to the DATASETS_PATH . Once you have configured the backend, below command will create a daemon and keeps the app running restarts on internal app crashes, read more here . !-- go into the project folder -- cd project_directory !-- start the server using pm2 -- pm2 start server.js --name APIServer Serving the API on HTTPS # To enable HTTPS during deployment, replace the entire code in server.js with below code: // importing the app const app = require( ./app ); // requiring the https, fs (node standard modules) const https = require( https ); const fs = require( fs ); // load configuration through environment variables from .env to process.env require( dotenv ).config(); // add the certificate for https var options = { key: fs.readFileSync( location_to_your_key_file ), cert: fs.readFileSync( location_to_your_cert_file ) }; // start the server, listening at the configured port. var server = https.createServer(options, app).listen(process.env.NODE_PORT || 8080, function() { console.log( Express server listening on port + process.env.NODE_PORT || 8080); }); You need to update the options property in the above code with server specific certificate files, after requesting them from a certificate authority. Restart the app to apply changes. Read more about using certificates at Node HTTPS docs Docker Docker deployment # You can build an image for this app using the provided Dockerfile , but it is useless on its own, since this project requires a MongoDB database to connect store data. To achieve this we need to use the docker-compose.yml in conjunction to the Dockerfile . Building a Docker Image Before building a docker image, we need to update the .env file to contain configuration as below. This ensures proper communication between the mongodb instance and our app within a docker container. DB_URL= mongo:27017 DB_NAME= testDB PUBLIC_ENDPOINT= http://localhost:8081/samples/ IMAGE_URL= http://localhost:8081/images/ NODE_PORT=8081 Change the dataset path within the app.js !-- Replace below line in app.js-- app.use( /images , express.static( some_example_path )); !-- to below, before building the images -- app.use( /images , express.static( /srv/app/images )); Build the image docker build --tag=demobackend . you can change the tag name from demobackend to anything you like, but make sure you also update the name in the docker-compose.yml file. To run the app use the command: docker-compose up To insert example data use the postData.py script within the utils folder: cd ./sampleData python postData.py example.json Known issues When you stop and start the containerized app, the data that was inserted into the db will be lost, to solve this problem docker uses volumes . MacOSX mongodb-container volume problem work around . Updating dependencies # To get a list of packages that are outdated, execute the below command from the project root directory npm outdated From your project root directory, run the update command npm update Updating dependencies sometimes breaks the app, which is expected and common software development. Refer the changelog for the packages that are updated to fix any issues. More details on above command usage : npm docs Extending the app # Recommend using Postman to develop, test and extend existing APIs.","title":"Backend Deployment"},{"location":"Backend Deployment/#backend-deployment","text":"Instructions to deploy web apps built using NodeJS on PSU's VM hosting servers.","title":"Backend Deployment"},{"location":"Backend Deployment/#deploying-the-application","text":"Below instructions are for a server running CentOS . However, app related configuration is still valid for any linux distribution. Install dependencies on the Server Install node (stable version) for CentOS , here is a tutorial . Install mongodb on the CentOS server. Install pm2 a production process manager for Node.js applications with a built-in load balancer. Server Specific Configuration Create a .env file. Add and edit below fields to appropriate values : DB_URL= DB_NAME= DATASETS_PATH= PUBLIC_ENDPOINT= IMAGE_URL= NODE_PORT= For example, a .env for a server example.vmhost.psu.edu by a user bob looks like below: DB_URL= localhost DB_NAME= testDB DATASETS_PATH= /home/bob/imageAssets PUBLIC_ENDPOINT= https://example.vmhost.psu.edu:8081/samples/ IMAGE_URL= https://example.vmhost.psu.edu:8081/images/ NODE_PORT=8081 Copy your files, images and other assets to the DATASETS_PATH . Once you have configured the backend, below command will create a daemon and keeps the app running restarts on internal app crashes, read more here . !-- go into the project folder -- cd project_directory !-- start the server using pm2 -- pm2 start server.js --name APIServer","title":"Deploying the application"},{"location":"Backend Deployment/#serving-the-api-on-https","text":"To enable HTTPS during deployment, replace the entire code in server.js with below code: // importing the app const app = require( ./app ); // requiring the https, fs (node standard modules) const https = require( https ); const fs = require( fs ); // load configuration through environment variables from .env to process.env require( dotenv ).config(); // add the certificate for https var options = { key: fs.readFileSync( location_to_your_key_file ), cert: fs.readFileSync( location_to_your_cert_file ) }; // start the server, listening at the configured port. var server = https.createServer(options, app).listen(process.env.NODE_PORT || 8080, function() { console.log( Express server listening on port + process.env.NODE_PORT || 8080); }); You need to update the options property in the above code with server specific certificate files, after requesting them from a certificate authority. Restart the app to apply changes. Read more about using certificates at Node HTTPS docs","title":"Serving the API on HTTPS"},{"location":"Backend Deployment/#docker-docker-deployment","text":"You can build an image for this app using the provided Dockerfile , but it is useless on its own, since this project requires a MongoDB database to connect store data. To achieve this we need to use the docker-compose.yml in conjunction to the Dockerfile . Building a Docker Image Before building a docker image, we need to update the .env file to contain configuration as below. This ensures proper communication between the mongodb instance and our app within a docker container. DB_URL= mongo:27017 DB_NAME= testDB PUBLIC_ENDPOINT= http://localhost:8081/samples/ IMAGE_URL= http://localhost:8081/images/ NODE_PORT=8081 Change the dataset path within the app.js !-- Replace below line in app.js-- app.use( /images , express.static( some_example_path )); !-- to below, before building the images -- app.use( /images , express.static( /srv/app/images )); Build the image docker build --tag=demobackend . you can change the tag name from demobackend to anything you like, but make sure you also update the name in the docker-compose.yml file. To run the app use the command: docker-compose up To insert example data use the postData.py script within the utils folder: cd ./sampleData python postData.py example.json Known issues When you stop and start the containerized app, the data that was inserted into the db will be lost, to solve this problem docker uses volumes . MacOSX mongodb-container volume problem work around .","title":"Docker &amp; Docker deployment"},{"location":"Backend Deployment/#updating-dependencies","text":"To get a list of packages that are outdated, execute the below command from the project root directory npm outdated From your project root directory, run the update command npm update Updating dependencies sometimes breaks the app, which is expected and common software development. Refer the changelog for the packages that are updated to fix any issues. More details on above command usage : npm docs","title":"Updating dependencies"},{"location":"Backend Deployment/#extending-the-app","text":"Recommend using Postman to develop, test and extend existing APIs.","title":"Extending the app"},{"location":"FAQ/","text":"FAQ # Frequently Asked Questions Installation # Can planemo_init add additional features without generating the whole wrapper using the command line? # No. You must use planemo_init with all the options included, you cannot add additional cases to an already existing wrapper (xml file). Test # Does planemo t not work if you don't include all the input and output test files in the same directory? # Yes. you must include all the input and output files in order to test. You can also select specific wrappers for testing, such as planemo t your.xml then you only need to include the test files for your.xml wrapper. Debug # Where can I get help with debugging? # You can search for open planemo issues and get help here: https://github.com/galaxyproject/planemo/issues. It says 'internet not reachable' when I am connected to the internet. What can I do? # You need to set the curl connect timeout and curl max time in the config file which is allocated in the following path: $ cd ~ $ cd .sdkman/etc $ ls Open the config file and change the following: sdkman_curl_connect_timeout=20 sdkman_curl_max_time=0 It might be that the internet service provider is blocking the connection. You can resolve this by either using a different internet or by using a VPN. See the link for more details on this bug. Debug # This guide outlines some of the common warning messages and errors during the planemo installation testing procedures. Common Warnings # Example: .. WARNING: No tests found, most tools should define test cases. .. WARNING: No valid test(s) found. This indicates that there are no test cases written in the wrapper(.xml file). Common Errors # dbkey error: https://github.com/galaxyproject/planemo/issues/746 Cannot locate xUnit report: https://github.com/galaxyproject/planemo/issues/724 planemo couldn't find a target test-data directory: This means that there is no test-data directory created for planemo testing. Fix the issue by simply checking planemo l or create a test-data directory manually and copy all input output files. Version Conflicts # A lot of troubles with the initial setup comes from the version conflict of multiple tools and dependencies. One can find the list of compatible versions below: Python 3.6.5 Planemo 0.53.0 Galaxy","title":"FAQ"},{"location":"FAQ/#faq","text":"Frequently Asked Questions","title":"FAQ"},{"location":"FAQ/#installation","text":"","title":"Installation"},{"location":"FAQ/#can-planemo_init-add-additional-features-without-generating-the-whole-wrapper-using-the-command-line","text":"No. You must use planemo_init with all the options included, you cannot add additional cases to an already existing wrapper (xml file).","title":"Can planemo_init add additional features without generating the whole wrapper using the command line?"},{"location":"FAQ/#test","text":"","title":"Test"},{"location":"FAQ/#does-planemo-t-not-work-if-you-dont-include-all-the-input-and-output-test-files-in-the-same-directory","text":"Yes. you must include all the input and output files in order to test. You can also select specific wrappers for testing, such as planemo t your.xml then you only need to include the test files for your.xml wrapper.","title":"Does planemo t not work if you don't include all the input and output test files in the same directory?"},{"location":"FAQ/#debug","text":"","title":"Debug"},{"location":"FAQ/#where-can-i-get-help-with-debugging","text":"You can search for open planemo issues and get help here: https://github.com/galaxyproject/planemo/issues.","title":"Where can I get help with debugging?"},{"location":"FAQ/#it-says-internet-not-reachable-when-i-am-connected-to-the-internet-what-can-i-do","text":"You need to set the curl connect timeout and curl max time in the config file which is allocated in the following path: $ cd ~ $ cd .sdkman/etc $ ls Open the config file and change the following: sdkman_curl_connect_timeout=20 sdkman_curl_max_time=0 It might be that the internet service provider is blocking the connection. You can resolve this by either using a different internet or by using a VPN. See the link for more details on this bug.","title":"It says 'internet not reachable' when I am connected to the internet. What can I do?"},{"location":"FAQ/#debug_1","text":"This guide outlines some of the common warning messages and errors during the planemo installation testing procedures.","title":"Debug"},{"location":"FAQ/#common-warnings","text":"Example: .. WARNING: No tests found, most tools should define test cases. .. WARNING: No valid test(s) found. This indicates that there are no test cases written in the wrapper(.xml file).","title":"Common Warnings"},{"location":"FAQ/#common-errors","text":"dbkey error: https://github.com/galaxyproject/planemo/issues/746 Cannot locate xUnit report: https://github.com/galaxyproject/planemo/issues/724 planemo couldn't find a target test-data directory: This means that there is no test-data directory created for planemo testing. Fix the issue by simply checking planemo l or create a test-data directory manually and copy all input output files.","title":"Common Errors"},{"location":"FAQ/#version-conflicts","text":"A lot of troubles with the initial setup comes from the version conflict of multiple tools and dependencies. One can find the list of compatible versions below: Python 3.6.5 Planemo 0.53.0 Galaxy","title":"Version Conflicts"},{"location":"Frontend Deployment/","text":"Frontend Deployment # Instructions to deploy web apps built using ReactJS on PSU's VM hosting servers. Pre-requisites # Install the latest stable release of Node.js Deploying React Apps # All the react based projects were bootstrapped with Create React App . This includes YEP , PCRP and any app created using the cookie-cutter template on github. Local App Development # # clone the project repo git clone project_repo # change your directory cd project_repo # install dependencies npm install # start a local development server. npm start Above steps are sufficient to setup a working copy on your local machine to start extending the frontend applications. To deploy the app, we need to compile the application to generate a build/ directory. Create React App comes with webpack , a package bundler, pre-configured to generate the necessary files for deployment. Below instructions are adapted from official deployment docs and assumes you already have Apache web server set up configured on the server. Generating a build to deploy the website on MARS with API on Pluto # Add below line into your package.json file. /yep is where your website will be deployed to on the server. you can choose any name based on your project. Make sure it is consistent with everything that follows. homepage : https://mars.vmhost.psu.edu/yep , Change your React app configuration in Config.js , to contain settings similar as below. These settings change based on where you decide to host your backend and frontend. const settings = { apiURL : https://pluto.vmhost.psu.edu:8081 , appURL : https://mars.vmhost.psu.edu/yep , siteAvailability : private , samplesEndpoint : /reviewSamples , aliasesEndpoint : /aliases , sgdEndpoint : /sgdInfo , trackHubPrefix : http://genome.ucsc.edu/cgi-bin/hgTracks?db=sacCer3 hubUrl=http://www.bx.psu.edu/~giardine/yepHub/hub2/hub.txt textSize=12 sgdGene=dense hgt.labelWidth=25 centerLabels=off position=chrI%3A0-230218 } Inside app.js , add the router configuration to use the /yep as the project path. This is necessary so that the app can correctly resolve internal URLs and path. BrowserRouter basename= /yep Make sure you have the .htaccess file inside the public/ folder of your project. Otherwise, create one and add below directives. Options -MultiViews RewriteEngine On RewriteCond %{REQUEST_FILENAME} !-f RewriteRule ^ index.html [QSA,L] Compile the app to create the build/ folder using the below command. !-- generate a build -- npm run build !-- rename the directory -- build/ to yep/ !-- Login to the server -- ssh mars.vmhost.psu.edu -p 1855 !-- stop apache -- sudo systemctl httpd stop !-- move the previous build to archive, if it exists -- cd /var/www/html mv yep/ WEBSITE_ARCHIVE/yep_ YY_MM_DD !-- copy your yep/ directory to MARS at `/var/www/html` -- using FileZilla or scp ( your choice.) !-- Restart apache -- sudo systemctl httpd start Apache WebAccess configuration # All websites use react-router , a Client-side routing library. Hence, requires some server configuration (for both apache and nginx) before deploying to production. Here is an explanation from Official react deployment docs Apart from the above mentioned configuration, since VMhost servers at PSU enable webaccess (sometimes by default ) and the way apache is set up by default has some restrictions in using .htaccess file that is included with these websites. Below are the steps to configure apache on your vm to make an exception for this website to use .htaccess files. !-- stop the apache server -- sudo service httpd stop !-- check the status of your apache server (to check if it is stopped) -- sudo service httpd status !-- edit the httpd (apache's) configuration (usually located at below location) -- cd /etc/httpd/conf/ sudo nano httpd.conf Look for the lines such as below, in the httpd.conf # Further relax access to the default document root: Directory /var/www/html # # Possible values for the Options directive are None , All , # or any combination of: # Indexes Includes FollowSymLinks SymLinksifOwnerMatch ExecCGI MultiViews # # Note that MultiViews must be named *explicitly* --- Options All # doesn't give it to you. # # The Options directive is both complicated and important. Please see # http://httpd.apache.org/docs/2.4/mod/core.html#options # for more information. # Options Indexes MultiViews FollowSymLinks # # AllowOverride controls what directives may be placed in .htaccess files. # It can be All , None , or any combination of the keywords: # Options FileInfo AuthConfig Limit # AllowOverride None # # Controls who can get stuff from this server. # Require all granted /Directory What is happening in the above directive is that, .htaccess files are being suppressed within each subfolder in apache 's root directory. for example, in the apache root directory, which is /var/www/html you have a folder called my_website then the .htaccess file within this folder is being ignored. Now add these directives into httpd.conf AFTER the above directives, to allow apache to read .htaccess in our website's folder. Directory /var/www/html/yep Options Indexes FollowSymLinks AllowOverride All Require all granted /Directory Start the server and you should have your website up and running. sudo service httpd start Website should be live at : https://mars.vmhost.psu.edu/yep/ Ignoring your Website from WebAccess # Add these directives to httpd.conf . Make sure you modify /yep in the below directives based on the folder name you are trying to exclude from WebAccess. Location /yep CosignProtected On CosignAllowPublicAccess On AuthType Cosign #Require valid-user Allow from all Satisfy any /Location Testing Website's performance # Use Google's Lighthouse to improve quality of your web app. Go to WebPageTest and plug in the URL to run common performance tests. Chrome Developer video (8:14) for a demo of this tool in action. Docker Docker deployment # Building a Docker Image Before building an image, you need change the app configuration specific to Docker: Config.js const settings = { apiURL : http://localhost:8080 , appURL : http://localhost:3000 , siteAvailability : private , samplesEndpoint : /reviewSamples , aliasesEndpoint : /aliases , sgdEndpoint : /sgdInfo , trackHubPrefix : http://genome.ucsc.edu/cgi-bin/hgTracks?db=sacCer3 hubUrl=http://www.bx.psu.edu/~giardine/yepHub/hub2/hub.txt textSize=12 sgdGene=dense hgt.labelWidth=25 centerLabels=off position=chrI%3A0-230218 } Compile and create the build/ npm run build Once you have the app configured, build the image locally docker build --tag=yepfrontend . Tag the image before pushing it to Dockerhub docker tag image-name username/repository:versiontag example : docker tag yepfrontend prashantkuntala/yepfrontend:v1.0 Login to Dockerhub docker login Push the Image docker push username/repository:versiontag example : docker push prashantkuntala/yepfrontend:v1.0 Inspecting Docker Images that are available on Dockerhub Go to MicroBadger and paste any image name available on Dockerhub to inspect it. Learn More # Create React App documentation Code Splitting: https://facebook.github.io/create-react-app/docs/code-splitting Analyzing the Bundle Size: https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size Making a Progressive Web App: https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app Advanced Configuration: https://facebook.github.io/create-react-app/docs/advanced-configuration Deployment: https://facebook.github.io/create-react-app/docs/deployment npm run build fails to minify : https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify","title":"Frontend Deployment"},{"location":"Frontend Deployment/#frontend-deployment","text":"Instructions to deploy web apps built using ReactJS on PSU's VM hosting servers.","title":"Frontend Deployment"},{"location":"Frontend Deployment/#pre-requisites","text":"Install the latest stable release of Node.js","title":"Pre-requisites"},{"location":"Frontend Deployment/#deploying-react-apps","text":"All the react based projects were bootstrapped with Create React App . This includes YEP , PCRP and any app created using the cookie-cutter template on github.","title":"Deploying React Apps"},{"location":"Frontend Deployment/#local-app-development","text":"# clone the project repo git clone project_repo # change your directory cd project_repo # install dependencies npm install # start a local development server. npm start Above steps are sufficient to setup a working copy on your local machine to start extending the frontend applications. To deploy the app, we need to compile the application to generate a build/ directory. Create React App comes with webpack , a package bundler, pre-configured to generate the necessary files for deployment. Below instructions are adapted from official deployment docs and assumes you already have Apache web server set up configured on the server.","title":"Local App Development"},{"location":"Frontend Deployment/#generating-a-build-to-deploy-the-website-on-mars-with-api-on-pluto","text":"Add below line into your package.json file. /yep is where your website will be deployed to on the server. you can choose any name based on your project. Make sure it is consistent with everything that follows. homepage : https://mars.vmhost.psu.edu/yep , Change your React app configuration in Config.js , to contain settings similar as below. These settings change based on where you decide to host your backend and frontend. const settings = { apiURL : https://pluto.vmhost.psu.edu:8081 , appURL : https://mars.vmhost.psu.edu/yep , siteAvailability : private , samplesEndpoint : /reviewSamples , aliasesEndpoint : /aliases , sgdEndpoint : /sgdInfo , trackHubPrefix : http://genome.ucsc.edu/cgi-bin/hgTracks?db=sacCer3 hubUrl=http://www.bx.psu.edu/~giardine/yepHub/hub2/hub.txt textSize=12 sgdGene=dense hgt.labelWidth=25 centerLabels=off position=chrI%3A0-230218 } Inside app.js , add the router configuration to use the /yep as the project path. This is necessary so that the app can correctly resolve internal URLs and path. BrowserRouter basename= /yep Make sure you have the .htaccess file inside the public/ folder of your project. Otherwise, create one and add below directives. Options -MultiViews RewriteEngine On RewriteCond %{REQUEST_FILENAME} !-f RewriteRule ^ index.html [QSA,L] Compile the app to create the build/ folder using the below command. !-- generate a build -- npm run build !-- rename the directory -- build/ to yep/ !-- Login to the server -- ssh mars.vmhost.psu.edu -p 1855 !-- stop apache -- sudo systemctl httpd stop !-- move the previous build to archive, if it exists -- cd /var/www/html mv yep/ WEBSITE_ARCHIVE/yep_ YY_MM_DD !-- copy your yep/ directory to MARS at `/var/www/html` -- using FileZilla or scp ( your choice.) !-- Restart apache -- sudo systemctl httpd start","title":"Generating a build to deploy the website on MARS with API on Pluto"},{"location":"Frontend Deployment/#apache-webaccess-configuration","text":"All websites use react-router , a Client-side routing library. Hence, requires some server configuration (for both apache and nginx) before deploying to production. Here is an explanation from Official react deployment docs Apart from the above mentioned configuration, since VMhost servers at PSU enable webaccess (sometimes by default ) and the way apache is set up by default has some restrictions in using .htaccess file that is included with these websites. Below are the steps to configure apache on your vm to make an exception for this website to use .htaccess files. !-- stop the apache server -- sudo service httpd stop !-- check the status of your apache server (to check if it is stopped) -- sudo service httpd status !-- edit the httpd (apache's) configuration (usually located at below location) -- cd /etc/httpd/conf/ sudo nano httpd.conf Look for the lines such as below, in the httpd.conf # Further relax access to the default document root: Directory /var/www/html # # Possible values for the Options directive are None , All , # or any combination of: # Indexes Includes FollowSymLinks SymLinksifOwnerMatch ExecCGI MultiViews # # Note that MultiViews must be named *explicitly* --- Options All # doesn't give it to you. # # The Options directive is both complicated and important. Please see # http://httpd.apache.org/docs/2.4/mod/core.html#options # for more information. # Options Indexes MultiViews FollowSymLinks # # AllowOverride controls what directives may be placed in .htaccess files. # It can be All , None , or any combination of the keywords: # Options FileInfo AuthConfig Limit # AllowOverride None # # Controls who can get stuff from this server. # Require all granted /Directory What is happening in the above directive is that, .htaccess files are being suppressed within each subfolder in apache 's root directory. for example, in the apache root directory, which is /var/www/html you have a folder called my_website then the .htaccess file within this folder is being ignored. Now add these directives into httpd.conf AFTER the above directives, to allow apache to read .htaccess in our website's folder. Directory /var/www/html/yep Options Indexes FollowSymLinks AllowOverride All Require all granted /Directory Start the server and you should have your website up and running. sudo service httpd start Website should be live at : https://mars.vmhost.psu.edu/yep/","title":"Apache WebAccess configuration"},{"location":"Frontend Deployment/#ignoring-your-website-from-webaccess","text":"Add these directives to httpd.conf . Make sure you modify /yep in the below directives based on the folder name you are trying to exclude from WebAccess. Location /yep CosignProtected On CosignAllowPublicAccess On AuthType Cosign #Require valid-user Allow from all Satisfy any /Location","title":"Ignoring your Website from WebAccess"},{"location":"Frontend Deployment/#testing-websites-performance","text":"Use Google's Lighthouse to improve quality of your web app. Go to WebPageTest and plug in the URL to run common performance tests. Chrome Developer video (8:14) for a demo of this tool in action.","title":"Testing Website's performance"},{"location":"Frontend Deployment/#docker-docker-deployment","text":"Building a Docker Image Before building an image, you need change the app configuration specific to Docker: Config.js const settings = { apiURL : http://localhost:8080 , appURL : http://localhost:3000 , siteAvailability : private , samplesEndpoint : /reviewSamples , aliasesEndpoint : /aliases , sgdEndpoint : /sgdInfo , trackHubPrefix : http://genome.ucsc.edu/cgi-bin/hgTracks?db=sacCer3 hubUrl=http://www.bx.psu.edu/~giardine/yepHub/hub2/hub.txt textSize=12 sgdGene=dense hgt.labelWidth=25 centerLabels=off position=chrI%3A0-230218 } Compile and create the build/ npm run build Once you have the app configured, build the image locally docker build --tag=yepfrontend . Tag the image before pushing it to Dockerhub docker tag image-name username/repository:versiontag example : docker tag yepfrontend prashantkuntala/yepfrontend:v1.0 Login to Dockerhub docker login Push the Image docker push username/repository:versiontag example : docker push prashantkuntala/yepfrontend:v1.0 Inspecting Docker Images that are available on Dockerhub Go to MicroBadger and paste any image name available on Dockerhub to inspect it.","title":"Docker &amp; Docker deployment"},{"location":"Frontend Deployment/#learn-more","text":"Create React App documentation Code Splitting: https://facebook.github.io/create-react-app/docs/code-splitting Analyzing the Bundle Size: https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size Making a Progressive Web App: https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app Advanced Configuration: https://facebook.github.io/create-react-app/docs/advanced-configuration Deployment: https://facebook.github.io/create-react-app/docs/deployment npm run build fails to minify : https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify","title":"Learn More"},{"location":"galaxy/","text":"Dependencies # Assuming you have installed the required development tools for Mac OS Install samtools . brew install samtools Install yarn . brew install yarn Installing Galaxy # It is recommended that you clone galaxy to your ~/Desktop , so that GALAXY's internal paths are shorter and avoid conda throwing any errors that your path exceeds the maximum characters for tool install paths here we are going to work with 17.09 release of GALAXY. Open a terminal and clone GALAXY to your ~/Desktop . cd ~/Desktop git clone -b release_17.09 https://github.com/galaxyproject/galaxy.git Start GALAXY cd galaxy sh run.sh Note that when you are starting galaxy for the first time, it takes some time to install required internal dependencies and setup its internal database. Once its gone through it, you should be able to see the start up page. Congratulations you have successfully installed GALAXY on your local machine. Useful resources tutorials Get galaxy Learn galaxy Dagobah training Configuring Galaxy # Setting up an Admin galaxy.ini file contains most of the configurations to your local galaxy. It has some predefined defaults which you might want to re-think before changing anything. We will be using cegr@psu.edu as the default admin, so that it can be used to integrate with PEGR in the PEGR tutorial, where we use our local galaxy to send out information to PEGR Make sure your Galaxy is not online, before making any changes to configuration. Open your galaxy folder in Finder on your mac. you should see a config/ folder. Inside the config/ , you will find a galaxy.ini.sample file. Copy the galaxy.ini.sample within the same folder and rename the copied file to galaxy.ini Open the galaxy.ini in a text editor of your choice. Search for the line that starts with admin_users and add cegr@psu.edu Above image shows the change. make sure you have edited ~/Desktop/galaxy/config/galaxy.ini Start your Galaxy now, sh run.sh Click on User menu and then click register You should see a registration page as below Enter cegr@psu.edu as the email address and choose a password , public name of your choice. Once you login, you should now have the Admin menu show up on the menu bar on the top. When you click on the Admin tab, you should see the below page. Adding Custom Genomes # sacCer3_cegr is the customized yeast genome we are using within the lab. Though the differences are few (to the best of my knowledge), but are very important to keep in mind while performing general data analysis. This genome deviates from the UCSC recommendations sacCer3_cegr contains 2-micron regions chromosome naming is using decimal number instead of roman numerals. This causes some disadvantages to use tools like bedGraphToBigWig, etc. Click on Admin tab, then under Tools and Tool Shed section, click on Search Tool Shed Click on Galaxy Main Tool Shed Search Install below tools. you can copy and paste below tool names into the search box. Tool Name data_manager_bwa_mem_index_builder data_manager_fetch_genome_dbkeys_all_fasta data_manager_sam_fasta_index_builder data_manager_twobit_builder Below images previews the steps for installing a tool. Once you have installed all the above tools, You can verify the installation at Admin Tools and Tool shed Manage installed tools Download the sacCer3_cegr genome from here Upload the file you downloaded above into galaxy, using the upload button located on the tools menu, as shown in the image below. Once you have uploaded, you should see the file appear in the \u201chistory pane\u201d on your right. Go to Admin tab, under Data section, Click on Local data to get Data manager We need to run below tools one after the other in the exact order mentioned below: Tool Order Create DBkey and reference geneome #1 TwoBit #2 BWA-MEM #3 SAM Fasta #4 All these tools are available from the Admin Data Local data section. Below are images for step-by-step execution of above tools in the same order. use them to fill out any default information that is required and follow along You can specify sequence name to be sacCer3_cegr and leave everything as default in all the tools. Once you have run all the tools, you need to check couple of things that populate in the internal database of galaxy. You can check that information from your Admin page. Go to Admin Data Local data section, under View Tool Data Table Entries . Click on __dbkeys__ you should see something like below. Similarly check all_fasta , twobit , bwa_mem_indexes . All of them should contain an entry for sacCer3_cegr . Path could be different in your case. If your able to see similar results as above images. Congratulations! you have successfully added a custom genome build into your galaxy. Importing ChIP-exo Workflow # We will install the core-sequencing workflow that the lab uses to analyze all the samples that are sequenced. This pipeline is run to create BAM files, peak calling using genetrack and MEME motif analysis. Download the workflow file here. Once you have downloaded the workflow. You can import it into your local galaxy from the Workflow tab using the upload or import workflow button located beside the search bar. (see image below). Once you have selected the workflow file and clicked import . You might see some errors such as below. Nothing to worry, the error messages is caused by tools that are not yet installed are important for the workflow to run in your galaxy. Click on edit option under the workflow drop-down menu. To find out the missing tools. You need to install each tool manually from toolshed. Go to Admin Tools and Tool Shed Search toolShed to search and install each tool that is missing. Few tools have their toolnames in the workflow that end with output_statistics . These are the tools that are not available on Galaxy toolshed and need to be side-loaded separately, which we will do in the next section. so for now, you can ignore installing these tools and their errors. Integrating CEGR output_statistics # This section is similar to adding custom tools into Galaxy, here is a ( tutorial ). cegr-galaxy repo contains other important scripts that are used to run the core-sequencing pipeline on production galaxy. The repo has a README file detailing the usage of each script. Clone the repository containing the CEGR tools from seqcode/cegr-galaxy git clone https://github.com/seqcode/cegr-galaxy.git Copy the entire cegr_statistics folder to this location galaxy/tools/ within your local galaxy. Open galaxy/config/tool_conf.xml in a text editor of your choice. If the above file doesn't exist, there should be a file in the same config directory called tool_conf.xml.main , copy and rename the file to tool_conf.xml Add below lines at the end of file, within the /toolbox tag. section id= cegr_tools name= CEGR tool file= cegr_statistics/bam_to_scidx_output_stats.xml / tool file= cegr_statistics/bedtools_intersectbed_output_stats.xml / tool file= cegr_statistics/bwa_mem_output_stats_single.xml / tool file= cegr_statistics/cwpair2_output_stats.xml / tool file= cegr_statistics/extract_genomic_dna_output_stats.xml / tool file= cegr_statistics/extract_genomic_dna_output_stats2.xml / tool file= cegr_statistics/extract_genomic_dna_output_stats3.xml / tool file= cegr_statistics/fasta_nucleotide_color_plot_output_stats.xml / tool file= cegr_statistics/fastqc_output_stats.xml / tool file= cegr_statistics/fastqc_output_stats2.xml / tool file= cegr_statistics/genetrack_output_stats.xml / tool file= cegr_statistics/input_dataset_r1_output_stats.xml / tool file= cegr_statistics/input_dataset_r2_output_stats.xml / tool file= cegr_statistics/mark_duplicates_bam_output_stats.xml / tool file= cegr_statistics/meme_fimo_output_stats.xml / tool file= cegr_statistics/meme_meme_output_stats.xml / tool file= cegr_statistics/pe_histogram_output_stats.xml / tool file= cegr_statistics/repeatmasker_wrapper_output_stats.xml / tool file= cegr_statistics/repeatmasker_wrapper_output_stats2.xml / tool file= cegr_statistics/samtool_filter2_output_stats.xml / tool file= cegr_statistics/tag_pileup_frequency_output_stats.xml / /section The above lines informs GALAXY, where it can find the tools and corresponding toolwrappers The file galaxy/config/tool_config.xml should look something like below: Save the file and restart Galaxy. You should now see these tools appear under Tools menu within galaxy's Analyze Data tab similar to below. Congratulations! you have successfully installed output_statistics tools into your galaxy. Connecting Galaxy to PEGR # Galaxy and PEGR communicate with each other using API keys. If you have not setup a local developmental PEGR. Set it up using these instructions come back to this section In the above section we installed output_statistics tools. These are the tools that send back information to PEGR . We did not configure the tools in the above section, which we will do in this section. Changing Galaxy's port First, we will configure Galaxy to run on a different port so that it does not conflict with PEGR 's default port. Open galaxy/config/galaxy.ini in a text editor and search for port setting. Change the port to 8090 . Below image shows the change in galaxy.ini . Generating Galaxy API key Click on Admin tab, under User Management section, click on API keys . Click on Generate button to create an API key. Don't regenerate a new key, if you already have one at this point. CEGR output_statistics configuration Open galaxy/tools/stats_config.ini.sample and add below information # Configuration file for the CEGR Galaxy ChIP-exo statistics tools. [defaults] # This section contains default settings for command line parameters that # can be overridden when they are passed to executed scripts. PEGR_API_KEY = REPLACE THIS WITH PEGR API KEY PEGR_URL = http://localhost:8080/pegr/api/stats GALAXY_API_KEY = REPLACE THIS WITH YOUR GALAXY ADMIN USER's API KEY GALAXY_BASE_URL = http://localhost:8090 [tool_categories] input_dataset_r1 = output_fastqRead1 input_dataset_r2 = output_fastqRead2 toolshed.g2.bx.psu.edu/repos/iuc/bam_to_scidx/bam_to_scidx/1.0.1 = output_bamToScidx toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_intersectbed/2.27.0.0 = output_bedtoolsIntersect toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa_mem/0.7.17.1 = output_bwaMem toolshed.g2.bx.psu.edu/repos/iuc/cwpair2/cwpair2/1.1.0 = output_cwpair2 toolshed.g2.bx.psu.edu/repos/iuc/genetrack/genetrack/1.0.1 = output_genetrack toolshed.g2.bx.psu.edu/repos/iuc/extract_genomic_dna/Extract genomic DNA 1/3.0.3 = output_extractGenomicDNA toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/0.70 = output_fastqc toolshed.g2.bx.psu.edu/repos/iuc/pe_histogram/pe_histogram/1.0.1 = output_peHistogram toolshed.g2.bx.psu.edu/repos/bgruening/repeat_masker/repeatmasker_wrapper/0.1.2 =output_repeatMasker toolshed.g2.bx.psu.edu/repos/iuc/meme_meme/meme_meme/4.11.2.0 = output_meme toolshed.g2.bx.psu.edu/repos/iuc/fasta_nucleotide_color_plot/fasta_nucleotide_color_plot/1.0.1 = output_fourColorPlot toolshed.g2.bx.psu.edu/repos/jjohnson/samtools_filter/samtools_filter/1.1.1 = output_samtoolFilter toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_MarkDuplicates/2.7.1.1 = output_markDuplicates toolshed.g2.bx.psu.edu/repos/iuc/meme_fimo/meme_fimo/4.11.2.0 = output_fimo toolshed.g2.bx.psu.edu/repos/iuc/tag_pileup_frequency/tag_pileup_frequency/1.0.1 = output_tagPileup Below is an example configuration, after you add your API Keys. Rename stats_config.ini.sample to stats_config.ini Install bioblend bioblend is a python library to interact with Galaxy, using APIs Installation instructions here If you have already installed Anaconda pip pip install bioblend Using bioblend docs Adding core-sequencing workflowId into your local development PEGRdb This step is important, as it lets PEGR know which workflow stats to accept and update on PEGR web frontend Download getWorkflowid.py from here Open the script and replace the url and key with your local galaxy url and API key respectively. After adding your API-key, looks like the below image: Start your galaxy Run the script to retrieve the workflowId. Below is an image showing expected output Your workflow id is ebfb8f50c6abde6d in the above example. Start your MySQL server and log into it using the username and password that you used while setting up PEGR or you can login as root mysql -u root -p use pegr; describe pipeline; Edit below query with a pipeline-name of your choice and add the workflowid you retrieved from above script and execute it. Below is an example query that you need insert into pipeline( version,name,note,pipeline_version,steps,workflow_id) values( 1, ' pipeline-name ', 'Locally installed galaxy sending JSON dictionaires to locally installed pegr','001', '[[ input_dataset_r1_output_stats , fastqRead1 ],[ input_dataset_r2_output_stats , fastqRead2 ],[ fastqc_output_stats , fastqc ],[ fastqc_output_stats2 , fastqc ],[ mark_duplicates_bam_output_stats , markDuplicates ],[ samtool_filter2_output_stats , samtoolFilter ],[ pe_histogram_output_stats , peHistogram ],[ bam_to_scidx_output_stats , bamToScidx ],[ genetrack_output_stats , genetrack ],[ bedtools_intersectbed_output_stats , bedtoolsIntersect ],[ cwpair2_output_stats , cwpair2 ],[ extract_genomic_dna_output_stats , extractGenomicDNA ],[ extract_genomic_dna_output_stats2 , extractGenomicDNA ],[ repeatmasker_wrapper_output_stats , repeatMasker ],[ repeatmasker_wrapper_output_stats2 , repeatMasker ],[ meme_meme_output_stats , meme ],[ meme_fimo_output_stats , fimo ],[ extract_genomic_dna_output_stats3 , extractGenomicDNA ],[ fasta_nucleotide_color_plot_output_stats , fourColorPlot ],[ tag_pileup_frequency_output_stats , tagPileup ]]' , workflowid ); This how your query looks like after adding your pipeline-name and workflowid You are all set for executing the pipeline. (if you have followed and set up the keys correctly, there should be no errors)","title":"Galaxy"},{"location":"galaxy/#dependencies","text":"Assuming you have installed the required development tools for Mac OS Install samtools . brew install samtools Install yarn . brew install yarn","title":"Dependencies"},{"location":"galaxy/#installing-galaxy","text":"It is recommended that you clone galaxy to your ~/Desktop , so that GALAXY's internal paths are shorter and avoid conda throwing any errors that your path exceeds the maximum characters for tool install paths here we are going to work with 17.09 release of GALAXY. Open a terminal and clone GALAXY to your ~/Desktop . cd ~/Desktop git clone -b release_17.09 https://github.com/galaxyproject/galaxy.git Start GALAXY cd galaxy sh run.sh Note that when you are starting galaxy for the first time, it takes some time to install required internal dependencies and setup its internal database. Once its gone through it, you should be able to see the start up page. Congratulations you have successfully installed GALAXY on your local machine. Useful resources tutorials Get galaxy Learn galaxy Dagobah training","title":"Installing Galaxy"},{"location":"galaxy/#configuring-galaxy","text":"Setting up an Admin galaxy.ini file contains most of the configurations to your local galaxy. It has some predefined defaults which you might want to re-think before changing anything. We will be using cegr@psu.edu as the default admin, so that it can be used to integrate with PEGR in the PEGR tutorial, where we use our local galaxy to send out information to PEGR Make sure your Galaxy is not online, before making any changes to configuration. Open your galaxy folder in Finder on your mac. you should see a config/ folder. Inside the config/ , you will find a galaxy.ini.sample file. Copy the galaxy.ini.sample within the same folder and rename the copied file to galaxy.ini Open the galaxy.ini in a text editor of your choice. Search for the line that starts with admin_users and add cegr@psu.edu Above image shows the change. make sure you have edited ~/Desktop/galaxy/config/galaxy.ini Start your Galaxy now, sh run.sh Click on User menu and then click register You should see a registration page as below Enter cegr@psu.edu as the email address and choose a password , public name of your choice. Once you login, you should now have the Admin menu show up on the menu bar on the top. When you click on the Admin tab, you should see the below page.","title":"Configuring Galaxy"},{"location":"galaxy/#adding-custom-genomes","text":"sacCer3_cegr is the customized yeast genome we are using within the lab. Though the differences are few (to the best of my knowledge), but are very important to keep in mind while performing general data analysis. This genome deviates from the UCSC recommendations sacCer3_cegr contains 2-micron regions chromosome naming is using decimal number instead of roman numerals. This causes some disadvantages to use tools like bedGraphToBigWig, etc. Click on Admin tab, then under Tools and Tool Shed section, click on Search Tool Shed Click on Galaxy Main Tool Shed Search Install below tools. you can copy and paste below tool names into the search box. Tool Name data_manager_bwa_mem_index_builder data_manager_fetch_genome_dbkeys_all_fasta data_manager_sam_fasta_index_builder data_manager_twobit_builder Below images previews the steps for installing a tool. Once you have installed all the above tools, You can verify the installation at Admin Tools and Tool shed Manage installed tools Download the sacCer3_cegr genome from here Upload the file you downloaded above into galaxy, using the upload button located on the tools menu, as shown in the image below. Once you have uploaded, you should see the file appear in the \u201chistory pane\u201d on your right. Go to Admin tab, under Data section, Click on Local data to get Data manager We need to run below tools one after the other in the exact order mentioned below: Tool Order Create DBkey and reference geneome #1 TwoBit #2 BWA-MEM #3 SAM Fasta #4 All these tools are available from the Admin Data Local data section. Below are images for step-by-step execution of above tools in the same order. use them to fill out any default information that is required and follow along You can specify sequence name to be sacCer3_cegr and leave everything as default in all the tools. Once you have run all the tools, you need to check couple of things that populate in the internal database of galaxy. You can check that information from your Admin page. Go to Admin Data Local data section, under View Tool Data Table Entries . Click on __dbkeys__ you should see something like below. Similarly check all_fasta , twobit , bwa_mem_indexes . All of them should contain an entry for sacCer3_cegr . Path could be different in your case. If your able to see similar results as above images. Congratulations! you have successfully added a custom genome build into your galaxy.","title":"Adding Custom Genomes"},{"location":"galaxy/#importing-chip-exo-workflow","text":"We will install the core-sequencing workflow that the lab uses to analyze all the samples that are sequenced. This pipeline is run to create BAM files, peak calling using genetrack and MEME motif analysis. Download the workflow file here. Once you have downloaded the workflow. You can import it into your local galaxy from the Workflow tab using the upload or import workflow button located beside the search bar. (see image below). Once you have selected the workflow file and clicked import . You might see some errors such as below. Nothing to worry, the error messages is caused by tools that are not yet installed are important for the workflow to run in your galaxy. Click on edit option under the workflow drop-down menu. To find out the missing tools. You need to install each tool manually from toolshed. Go to Admin Tools and Tool Shed Search toolShed to search and install each tool that is missing. Few tools have their toolnames in the workflow that end with output_statistics . These are the tools that are not available on Galaxy toolshed and need to be side-loaded separately, which we will do in the next section. so for now, you can ignore installing these tools and their errors.","title":"Importing ChIP-exo Workflow"},{"location":"galaxy/#integrating-cegr-output_statistics","text":"This section is similar to adding custom tools into Galaxy, here is a ( tutorial ). cegr-galaxy repo contains other important scripts that are used to run the core-sequencing pipeline on production galaxy. The repo has a README file detailing the usage of each script. Clone the repository containing the CEGR tools from seqcode/cegr-galaxy git clone https://github.com/seqcode/cegr-galaxy.git Copy the entire cegr_statistics folder to this location galaxy/tools/ within your local galaxy. Open galaxy/config/tool_conf.xml in a text editor of your choice. If the above file doesn't exist, there should be a file in the same config directory called tool_conf.xml.main , copy and rename the file to tool_conf.xml Add below lines at the end of file, within the /toolbox tag. section id= cegr_tools name= CEGR tool file= cegr_statistics/bam_to_scidx_output_stats.xml / tool file= cegr_statistics/bedtools_intersectbed_output_stats.xml / tool file= cegr_statistics/bwa_mem_output_stats_single.xml / tool file= cegr_statistics/cwpair2_output_stats.xml / tool file= cegr_statistics/extract_genomic_dna_output_stats.xml / tool file= cegr_statistics/extract_genomic_dna_output_stats2.xml / tool file= cegr_statistics/extract_genomic_dna_output_stats3.xml / tool file= cegr_statistics/fasta_nucleotide_color_plot_output_stats.xml / tool file= cegr_statistics/fastqc_output_stats.xml / tool file= cegr_statistics/fastqc_output_stats2.xml / tool file= cegr_statistics/genetrack_output_stats.xml / tool file= cegr_statistics/input_dataset_r1_output_stats.xml / tool file= cegr_statistics/input_dataset_r2_output_stats.xml / tool file= cegr_statistics/mark_duplicates_bam_output_stats.xml / tool file= cegr_statistics/meme_fimo_output_stats.xml / tool file= cegr_statistics/meme_meme_output_stats.xml / tool file= cegr_statistics/pe_histogram_output_stats.xml / tool file= cegr_statistics/repeatmasker_wrapper_output_stats.xml / tool file= cegr_statistics/repeatmasker_wrapper_output_stats2.xml / tool file= cegr_statistics/samtool_filter2_output_stats.xml / tool file= cegr_statistics/tag_pileup_frequency_output_stats.xml / /section The above lines informs GALAXY, where it can find the tools and corresponding toolwrappers The file galaxy/config/tool_config.xml should look something like below: Save the file and restart Galaxy. You should now see these tools appear under Tools menu within galaxy's Analyze Data tab similar to below. Congratulations! you have successfully installed output_statistics tools into your galaxy.","title":"Integrating CEGR output_statistics"},{"location":"galaxy/#connecting-galaxy-to-pegr","text":"Galaxy and PEGR communicate with each other using API keys. If you have not setup a local developmental PEGR. Set it up using these instructions come back to this section In the above section we installed output_statistics tools. These are the tools that send back information to PEGR . We did not configure the tools in the above section, which we will do in this section. Changing Galaxy's port First, we will configure Galaxy to run on a different port so that it does not conflict with PEGR 's default port. Open galaxy/config/galaxy.ini in a text editor and search for port setting. Change the port to 8090 . Below image shows the change in galaxy.ini . Generating Galaxy API key Click on Admin tab, under User Management section, click on API keys . Click on Generate button to create an API key. Don't regenerate a new key, if you already have one at this point. CEGR output_statistics configuration Open galaxy/tools/stats_config.ini.sample and add below information # Configuration file for the CEGR Galaxy ChIP-exo statistics tools. [defaults] # This section contains default settings for command line parameters that # can be overridden when they are passed to executed scripts. PEGR_API_KEY = REPLACE THIS WITH PEGR API KEY PEGR_URL = http://localhost:8080/pegr/api/stats GALAXY_API_KEY = REPLACE THIS WITH YOUR GALAXY ADMIN USER's API KEY GALAXY_BASE_URL = http://localhost:8090 [tool_categories] input_dataset_r1 = output_fastqRead1 input_dataset_r2 = output_fastqRead2 toolshed.g2.bx.psu.edu/repos/iuc/bam_to_scidx/bam_to_scidx/1.0.1 = output_bamToScidx toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_intersectbed/2.27.0.0 = output_bedtoolsIntersect toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa_mem/0.7.17.1 = output_bwaMem toolshed.g2.bx.psu.edu/repos/iuc/cwpair2/cwpair2/1.1.0 = output_cwpair2 toolshed.g2.bx.psu.edu/repos/iuc/genetrack/genetrack/1.0.1 = output_genetrack toolshed.g2.bx.psu.edu/repos/iuc/extract_genomic_dna/Extract genomic DNA 1/3.0.3 = output_extractGenomicDNA toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/0.70 = output_fastqc toolshed.g2.bx.psu.edu/repos/iuc/pe_histogram/pe_histogram/1.0.1 = output_peHistogram toolshed.g2.bx.psu.edu/repos/bgruening/repeat_masker/repeatmasker_wrapper/0.1.2 =output_repeatMasker toolshed.g2.bx.psu.edu/repos/iuc/meme_meme/meme_meme/4.11.2.0 = output_meme toolshed.g2.bx.psu.edu/repos/iuc/fasta_nucleotide_color_plot/fasta_nucleotide_color_plot/1.0.1 = output_fourColorPlot toolshed.g2.bx.psu.edu/repos/jjohnson/samtools_filter/samtools_filter/1.1.1 = output_samtoolFilter toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_MarkDuplicates/2.7.1.1 = output_markDuplicates toolshed.g2.bx.psu.edu/repos/iuc/meme_fimo/meme_fimo/4.11.2.0 = output_fimo toolshed.g2.bx.psu.edu/repos/iuc/tag_pileup_frequency/tag_pileup_frequency/1.0.1 = output_tagPileup Below is an example configuration, after you add your API Keys. Rename stats_config.ini.sample to stats_config.ini Install bioblend bioblend is a python library to interact with Galaxy, using APIs Installation instructions here If you have already installed Anaconda pip pip install bioblend Using bioblend docs Adding core-sequencing workflowId into your local development PEGRdb This step is important, as it lets PEGR know which workflow stats to accept and update on PEGR web frontend Download getWorkflowid.py from here Open the script and replace the url and key with your local galaxy url and API key respectively. After adding your API-key, looks like the below image: Start your galaxy Run the script to retrieve the workflowId. Below is an image showing expected output Your workflow id is ebfb8f50c6abde6d in the above example. Start your MySQL server and log into it using the username and password that you used while setting up PEGR or you can login as root mysql -u root -p use pegr; describe pipeline; Edit below query with a pipeline-name of your choice and add the workflowid you retrieved from above script and execute it. Below is an example query that you need insert into pipeline( version,name,note,pipeline_version,steps,workflow_id) values( 1, ' pipeline-name ', 'Locally installed galaxy sending JSON dictionaires to locally installed pegr','001', '[[ input_dataset_r1_output_stats , fastqRead1 ],[ input_dataset_r2_output_stats , fastqRead2 ],[ fastqc_output_stats , fastqc ],[ fastqc_output_stats2 , fastqc ],[ mark_duplicates_bam_output_stats , markDuplicates ],[ samtool_filter2_output_stats , samtoolFilter ],[ pe_histogram_output_stats , peHistogram ],[ bam_to_scidx_output_stats , bamToScidx ],[ genetrack_output_stats , genetrack ],[ bedtools_intersectbed_output_stats , bedtoolsIntersect ],[ cwpair2_output_stats , cwpair2 ],[ extract_genomic_dna_output_stats , extractGenomicDNA ],[ extract_genomic_dna_output_stats2 , extractGenomicDNA ],[ repeatmasker_wrapper_output_stats , repeatMasker ],[ repeatmasker_wrapper_output_stats2 , repeatMasker ],[ meme_meme_output_stats , meme ],[ meme_fimo_output_stats , fimo ],[ extract_genomic_dna_output_stats3 , extractGenomicDNA ],[ fasta_nucleotide_color_plot_output_stats , fourColorPlot ],[ tag_pileup_frequency_output_stats , tagPileup ]]' , workflowid ); This how your query looks like after adding your pipeline-name and workflowid You are all set for executing the pipeline. (if you have followed and set up the keys correctly, there should be no errors)","title":"Connecting Galaxy to PEGR"},{"location":"pegr/","text":"PEGR # Pre-Install # Install curl (For Windows, use Git Bash) Install sdkman ( tutorial ): $ curl -s \"https://get.sdkman.io\" | bash $ source \"$HOME/.sdkman/bin/sdkman-init.sh\" $ sdk version # check your sdkman version Install Grails: $ sdk install grails 2.5.5 $ sdk list grails # check your installation of grails $ sdk use grails 2.5.5 # change your default grails to version 2.5.4 Now close the terminal and quit the terminal in your dashboard. [Important!] Install Groovy: $ sdk install groovy 2.4.4 $ sdk list groovy # check your current version of groovy Install Java: $ sdk install java 7u141-zulu # install Java (specific version needed for current PEGR) $ sdk install java 8u141-zulu # alternate version that is also compatible $ sdk list java # Check your current version of java Install MAMP: * https://www.mamp.info/en/downloads/ # download link Now Close your terminal and quit the terminal on the dashboard.[Need to do above step, if you want to run pegr locally ;)] $ grails # check if grails has been successfully installed Press ctrl+C to exit the prompt. PEGR Install # \u2022 :smile: Go to any directory of your choice and clone the PEGR git repo. $ git clone https://github.com/seqcode/pegr.git # clone \"pegr\" in your directory \u2022 Navigate to pegr/pegr/grails-app/conf/BuildConfig.groovy and edit these lines grails.project.target.level = 1.6 - grails.project.target.level = 1.7 grails.project.source.level = 1.6 - grails.project.source.level = 1.7 Download the MySQL file and rename it to pegrDB.sql Go to System Preferences and find MySql icon, then click to start the SQL server. $ mysql.server start -(mac) if you have installed using homebrew Open the terminal and navigate to the same directory as the pegrDB.sql file $ mysql -u root -p # Connect to your server This will prompt you for that password it gave you. [ if you installed MySQL using the .dmg file] otherwise, in the case of homebrew, there is no password set for root. You should be able to see the prompt mysql After successfully connected to the MySQL, set the password for root $ mysql ALTER USER 'root'@'localhost' IDENTIFIED BY ' your new password '; \u2022 Create and switch to the new database: mysql Create database pegr; mysql use pegr; mysql source pegrDB.sql mysql show tables; \u2022 Successful output here Create a new user to access your \u201cpegr\u201d database mysql create user 'username'@'localhost' identified by 'password'; mysql grant all privileges on *.* to 'username'@'localhost'; For an example, mysql create user hyc5135@localhost identified by 'mypassword'; mysql grant all privileges on *.* to hyc5135@localhost; Modify an existing account to gain a login credentials (need a valid non-WebAccess account) Within your pegrDB, locate the user \"labadmin\" and replace the password value: $ mysql UPDATE user set password=\"password\" where username=\"labadmin\"; Open the BuildConfig.groovy file at pegr/grails-app/conf/BuildConfig.groovy and check the plugins block and verify that the build for the tomcat is appropriate to your version and add the below lines within the plugin block compile \":spring-security-core:2.0.0\" compile \"org.grails.plugins:quartz:1.0.1\" compile \"org.grails.plugins:mail:1.0.7\" Also, within the dependencies block, uncomment runtime 'mysql' [if it is commented] Now open DataSource.groovy at the pegr/grails-app/conf/DataSource.groovy and edit the datasource block to look like this: environments { development { dataSource { dbCreate = \"update\" // one of 'create', 'create-drop', 'update', 'validate', '' url = \"database_url\" username=\" your_username\" password=\"your_password\" } } } Now go into the pegr folder and run pegr $ grails run-app This will successfully start and provide with you an url (http://localhost:8081/pegr) that usually directs you to the login screen of PEGR. Login using the following labadmin credentials: Username: labadmin Password: passcode CONGRATULATIONS, YOU HAVE SUCCESSFULLY SET UP THE LOCAL PEGR! Grails Tutorial # \u2022 Tutorial link reference: http://grails.asia/ Introduction # What is Grails? Grails is an open source web application framework that uses the Apache Groovy programming language (based on Java). It is intended to be a high-productivity framework by following the \"coding by convention\" paradigm, providing a stand-alone development environment and hiding much of the configuration detail from the developer. Benefits # Convention over configuration ushers in abundant productivity for the developers. Grails enables you to write DRY code. If you have any existing Java code, reusing it in Grails should be no problem. Developers don\u2019t need to write all the plumbing/boilerplate code with Grails. Here, developers have a big relief as they can concentrate more on turning your ideas into applications instead of chasing after configuring the framework components. As it is free of XML configuration, the Groovy on Grails can help you to develop the application in real time. Grails supports scaffolding. This helps developers to create applications with CRUD functionalities- Create, Read, Update and delete. Simple and easy to maintain code. Migration # Grails 3.0 is a complete ground up rewrite of Grails and introduces new concepts and components for many parts of the framework. When upgrading an application or plugin from Grails 3.0 there are many areas to consider including: For step-by-step migration guide click [here] The migration of Grails app version 2 to 3 can be done with helpful links . To summarize the video, there are three main changes: Different file directories Merged files Deleted files There are also more changes as follows: Removal of dynamic scaffolding from Grails 3.0.0 till 3.0.4 when it was re-introduced Removal of before and after interceptors Project structure differences File location differences Configuration differences Package name differences Legacy Gant Scripts Gradle Build System Changes to Plugins Source vs Binary Plugins The biggest suggestion from the video is installing a fresh, new grails 3 app and migrating from the original grails 2 app instead of trying to make the changes in the original app. This will create a clean migration platform. After the migration, you must test intensively to catch any unexpected errors. PEGR Upgrade Notes (Grails 2.5.5 - 3.3.5) # Written by: Pierce Chaffin Last Updated: 06/27/18 This is not all inclusive of bugs you will encounter however I am working to add those as soon as I can Notes: Initial Migration of Files # First Step is to create a new grails project in grails 3.3.5 in a new directory.sdf Next is to migrate all relevant files to their new homes in the new file hierarchy in grails 3.x Start with the source files # $ cp -rf ../old_app/src/groovy/* src/main/groovy $ cp -rf ../old_app/src/java/* src/main/groovy $ cp -rf ../old_app/grails-app/* grails-app Then migrate all test files # $ cp -rf ../old_app/test/unit/* src/main/groovy $ cp -rf ../old_app/src/java/* src/main/groovy $ cp -rf ../old_app/grails-app/* grails-app Now that was it for the easy segments \u2026 onto all the configuration files and reorganization For starters lets migrate all dependencies from BuildConfig.groovy to the new build.gradle file (Mind you all of these plugins have new names and access locations as codehaus and other packages no longer exist) Ex: As the lib directory no longer is directly supported do this \u2026 compile fileTree(dir:'lib', include:'.jar') Additionally, going back to codehaus # org.codehaus.groovy.grails. has been migrated to grails.code.GrailsApplication.* # At this point in terminal we are going to want to resolve controller dependencies as well as some domain dependencies Do this by running # $ grails compile Do this quite a few times, looking at the stack trace and resolving as you go First issue you will probably encounter is that of @grails.validation.Validateable This is no longer in Grails 3 and needs to be changed to a class implementation class XXXX implements grails.validation.Validateable() A few other notes with regards to syntax \u2026 a few packages in grails have changed simply in terminology .. i.e. j_username is now simply username in Spring Security Core (auth.gsp) # Once you compile successfully you can try to run-app but almost assuredly will find that this doesn\u2019t work Next comes the configuration files \u2026 Start by moving URLMappings.groovy to the controllers directory And then change Config.groovy to application.groovy And then it is up to you (I went with the YAML file) as to where you want to merge in your DataSource.groovy (Either into application.yml or application.groovy Delete log4j from the application.groovy file Migrate the URLInterceptMapping to the new formatting and change it to a static rules mapping Run # $ grails s2-quickstart User Role Past this, a lot of functionality might still not work as some dependencies are still not lining up properly # At this point the concept of the security in Grails being different in defaults is the largest obstacle. In grails 3.x, by default all pages are inaccessible unless explicitly notated in the static rules of the application security for specific rules. Thus, as shown below make a mapping for every page with rules with regards to User Roles. [pattern:'/report/togglePreferredAlignment/**', access:['ROLE_ADMIN']],","title":"PEGR"},{"location":"pegr/#pegr","text":"","title":"PEGR"},{"location":"pegr/#pre-install","text":"Install curl (For Windows, use Git Bash) Install sdkman ( tutorial ): $ curl -s \"https://get.sdkman.io\" | bash $ source \"$HOME/.sdkman/bin/sdkman-init.sh\" $ sdk version # check your sdkman version Install Grails: $ sdk install grails 2.5.5 $ sdk list grails # check your installation of grails $ sdk use grails 2.5.5 # change your default grails to version 2.5.4 Now close the terminal and quit the terminal in your dashboard. [Important!] Install Groovy: $ sdk install groovy 2.4.4 $ sdk list groovy # check your current version of groovy Install Java: $ sdk install java 7u141-zulu # install Java (specific version needed for current PEGR) $ sdk install java 8u141-zulu # alternate version that is also compatible $ sdk list java # Check your current version of java Install MAMP: * https://www.mamp.info/en/downloads/ # download link Now Close your terminal and quit the terminal on the dashboard.[Need to do above step, if you want to run pegr locally ;)] $ grails # check if grails has been successfully installed Press ctrl+C to exit the prompt.","title":"Pre-Install"},{"location":"pegr/#pegr-install","text":"\u2022 :smile: Go to any directory of your choice and clone the PEGR git repo. $ git clone https://github.com/seqcode/pegr.git # clone \"pegr\" in your directory \u2022 Navigate to pegr/pegr/grails-app/conf/BuildConfig.groovy and edit these lines grails.project.target.level = 1.6 - grails.project.target.level = 1.7 grails.project.source.level = 1.6 - grails.project.source.level = 1.7 Download the MySQL file and rename it to pegrDB.sql Go to System Preferences and find MySql icon, then click to start the SQL server. $ mysql.server start -(mac) if you have installed using homebrew Open the terminal and navigate to the same directory as the pegrDB.sql file $ mysql -u root -p # Connect to your server This will prompt you for that password it gave you. [ if you installed MySQL using the .dmg file] otherwise, in the case of homebrew, there is no password set for root. You should be able to see the prompt mysql After successfully connected to the MySQL, set the password for root $ mysql ALTER USER 'root'@'localhost' IDENTIFIED BY ' your new password '; \u2022 Create and switch to the new database: mysql Create database pegr; mysql use pegr; mysql source pegrDB.sql mysql show tables; \u2022 Successful output here Create a new user to access your \u201cpegr\u201d database mysql create user 'username'@'localhost' identified by 'password'; mysql grant all privileges on *.* to 'username'@'localhost'; For an example, mysql create user hyc5135@localhost identified by 'mypassword'; mysql grant all privileges on *.* to hyc5135@localhost; Modify an existing account to gain a login credentials (need a valid non-WebAccess account) Within your pegrDB, locate the user \"labadmin\" and replace the password value: $ mysql UPDATE user set password=\"password\" where username=\"labadmin\"; Open the BuildConfig.groovy file at pegr/grails-app/conf/BuildConfig.groovy and check the plugins block and verify that the build for the tomcat is appropriate to your version and add the below lines within the plugin block compile \":spring-security-core:2.0.0\" compile \"org.grails.plugins:quartz:1.0.1\" compile \"org.grails.plugins:mail:1.0.7\" Also, within the dependencies block, uncomment runtime 'mysql' [if it is commented] Now open DataSource.groovy at the pegr/grails-app/conf/DataSource.groovy and edit the datasource block to look like this: environments { development { dataSource { dbCreate = \"update\" // one of 'create', 'create-drop', 'update', 'validate', '' url = \"database_url\" username=\" your_username\" password=\"your_password\" } } } Now go into the pegr folder and run pegr $ grails run-app This will successfully start and provide with you an url (http://localhost:8081/pegr) that usually directs you to the login screen of PEGR. Login using the following labadmin credentials: Username: labadmin Password: passcode CONGRATULATIONS, YOU HAVE SUCCESSFULLY SET UP THE LOCAL PEGR!","title":"PEGR Install"},{"location":"pegr/#grails-tutorial","text":"\u2022 Tutorial link reference: http://grails.asia/","title":"Grails Tutorial"},{"location":"pegr/#introduction","text":"What is Grails? Grails is an open source web application framework that uses the Apache Groovy programming language (based on Java). It is intended to be a high-productivity framework by following the \"coding by convention\" paradigm, providing a stand-alone development environment and hiding much of the configuration detail from the developer.","title":"Introduction"},{"location":"pegr/#benefits","text":"Convention over configuration ushers in abundant productivity for the developers. Grails enables you to write DRY code. If you have any existing Java code, reusing it in Grails should be no problem. Developers don\u2019t need to write all the plumbing/boilerplate code with Grails. Here, developers have a big relief as they can concentrate more on turning your ideas into applications instead of chasing after configuring the framework components. As it is free of XML configuration, the Groovy on Grails can help you to develop the application in real time. Grails supports scaffolding. This helps developers to create applications with CRUD functionalities- Create, Read, Update and delete. Simple and easy to maintain code.","title":"Benefits"},{"location":"pegr/#migration","text":"Grails 3.0 is a complete ground up rewrite of Grails and introduces new concepts and components for many parts of the framework. When upgrading an application or plugin from Grails 3.0 there are many areas to consider including: For step-by-step migration guide click [here] The migration of Grails app version 2 to 3 can be done with helpful links . To summarize the video, there are three main changes: Different file directories Merged files Deleted files There are also more changes as follows: Removal of dynamic scaffolding from Grails 3.0.0 till 3.0.4 when it was re-introduced Removal of before and after interceptors Project structure differences File location differences Configuration differences Package name differences Legacy Gant Scripts Gradle Build System Changes to Plugins Source vs Binary Plugins The biggest suggestion from the video is installing a fresh, new grails 3 app and migrating from the original grails 2 app instead of trying to make the changes in the original app. This will create a clean migration platform. After the migration, you must test intensively to catch any unexpected errors.","title":"Migration"},{"location":"pegr/#pegr-upgrade-notes-grails-255-335","text":"Written by: Pierce Chaffin Last Updated: 06/27/18 This is not all inclusive of bugs you will encounter however I am working to add those as soon as I can Notes:","title":"PEGR Upgrade Notes (Grails 2.5.5 -&gt; 3.3.5)"},{"location":"pegr/#initial-migration-of-files","text":"First Step is to create a new grails project in grails 3.3.5 in a new directory.sdf Next is to migrate all relevant files to their new homes in the new file hierarchy in grails 3.x","title":"Initial Migration of Files"},{"location":"pegr/#start-with-the-source-files","text":"$ cp -rf ../old_app/src/groovy/* src/main/groovy $ cp -rf ../old_app/src/java/* src/main/groovy $ cp -rf ../old_app/grails-app/* grails-app","title":"Start with the source files"},{"location":"pegr/#then-migrate-all-test-files","text":"$ cp -rf ../old_app/test/unit/* src/main/groovy $ cp -rf ../old_app/src/java/* src/main/groovy $ cp -rf ../old_app/grails-app/* grails-app Now that was it for the easy segments \u2026 onto all the configuration files and reorganization For starters lets migrate all dependencies from BuildConfig.groovy to the new build.gradle file (Mind you all of these plugins have new names and access locations as codehaus and other packages no longer exist) Ex: As the lib directory no longer is directly supported do this \u2026 compile fileTree(dir:'lib', include:'.jar')","title":"Then migrate all test files"},{"location":"pegr/#additionally-going-back-to-codehaus","text":"","title":"Additionally, going back to codehaus"},{"location":"pegr/#orgcodehausgroovygrails-has-been-migrated-to-grailscodegrailsapplication","text":"At this point in terminal we are going to want to resolve controller dependencies as well as some domain dependencies","title":"org.codehaus.groovy.grails. has been migrated to grails.code.GrailsApplication.*"},{"location":"pegr/#do-this-by-running","text":"$ grails compile Do this quite a few times, looking at the stack trace and resolving as you go First issue you will probably encounter is that of @grails.validation.Validateable This is no longer in Grails 3 and needs to be changed to a class implementation class XXXX implements grails.validation.Validateable()","title":"Do this by running"},{"location":"pegr/#a-few-other-notes-with-regards-to-syntax-a-few-packages-in-grails-have-changed-simply-in-terminology-ie-j_username-is-now-simply-username-in-spring-security-core-authgsp","text":"Once you compile successfully you can try to run-app but almost assuredly will find that this doesn\u2019t work Next comes the configuration files \u2026 Start by moving URLMappings.groovy to the controllers directory And then change Config.groovy to application.groovy And then it is up to you (I went with the YAML file) as to where you want to merge in your DataSource.groovy (Either into application.yml or application.groovy Delete log4j from the application.groovy file Migrate the URLInterceptMapping to the new formatting and change it to a static rules mapping","title":"A few other notes with regards to syntax \u2026 a few packages in grails have changed simply in terminology .. i.e. j_username is now simply username in Spring Security Core (auth.gsp)"},{"location":"pegr/#run","text":"$ grails s2-quickstart User Role","title":"Run"},{"location":"pegr/#past-this-a-lot-of-functionality-might-still-not-work-as-some-dependencies-are-still-not-lining-up-properly","text":"At this point the concept of the security in Grails being different in defaults is the largest obstacle. In grails 3.x, by default all pages are inaccessible unless explicitly notated in the static rules of the application security for specific rules. Thus, as shown below make a mapping for every page with rules with regards to User Roles. [pattern:'/report/togglePreferredAlignment/**', access:['ROLE_ADMIN']],","title":"Past this, a lot of functionality might still not work as some dependencies are still not lining up properly"},{"location":"planemo/","text":"Guidelines # Best Practices Standards Creating Galaxy tools Writing tool wrappers Tool Development Galaxy XML Tags What is Planemo ? Building Galaxy Tools Using Planemo Galaxy Development Training Slides Publishing to GALAXY Toolshed ToolShed Readiness Checklist Publishing tools from Planemo Make sure you refer to the above links while writing tools and their xml wrappers. It will be easy to submit tools to Intergalactic Utilities Commission IUC , if you followed the above standards. emoji cheatsheet reference used in this documentation. Install # Follow the instructions in the documentation to install on your machine. Using Planemo # Three step process that every tool need to go through : planemo lint your_tool.xml planemo test your_tool.xml --galaxy_root=[path to your local galaxy instance] --test_data [path to the directory containing your testdata] planemo serve your_tool.xml --galaxy_root=[path to your local galaxy instance] If you don't want to use (or) don't have your local galaxy for testing, Planemo will download spin-up a standalone galaxy to test your wrappers. make sure you installed Planemo within a python environment as instructed in installation documentation for Planemo . Each of the above commands supports --help option. use it for the entire list of supported options. If --no-xsd option is used for linting the tool wrapper, Planemo finds errors and warnings within your wrappers by ignoring the validation of correct XML schema. (read the documentation for more info, you might generally want to avoid using this option) planemo test expects test data to be available in a folder named as test-data . you need to write tool specific tests and also provide test data for testing the wrapper, before running the command. Writing functional tests for Tool Wrappers # References to writing proper tests Basic Tests Using tests /tests XML tag Advanced Tests Using reStructuredText to format help section within the tool wrapper - documentation Use the online restructedText editor to write and format your content under help /help section for the wrapper. planemo creates a tool_test_output.html file to showing the errors and debugging information if tools fail otherwise, shows the test output. planemo serve lets you visualize the tools wrapper in GALAXY instance once it passes the tests. Flake8 pycodestyle for python tools Python scripts must conform to pep8 standards and also pass flake8 , so that your tools pass the travisCI tests done by IUC . pycodestyle is the utility to test your python scripts for pep8 coding standards pycodestyle --show-pep8 --show-source --ignore=E501 your_tool .py For the case of submitting and testing the pep8 conformations, the IUC ignores some of the minor errors , such as E501, E201, E202. Refer the below issue on IUC, on what was actually proposed. tools-iuc/issue/467 I would install flake8 and flake8-import-order to do a simple test to remove any errors that pop up after pep8. Below is the command to test the flake8 on the scripts within the current directory. # installing flake8 and flake8-import-order pip install flake8 flake8-import-order # Testing for flake8 errors flake8 --ignore=E201,E202,E501 --count . flake8 documentation : here flake8-import-order : here pycodestyle documentation: here","title":"Planemo"},{"location":"planemo/#guidelines","text":"Best Practices Standards Creating Galaxy tools Writing tool wrappers Tool Development Galaxy XML Tags What is Planemo ? Building Galaxy Tools Using Planemo Galaxy Development Training Slides Publishing to GALAXY Toolshed ToolShed Readiness Checklist Publishing tools from Planemo Make sure you refer to the above links while writing tools and their xml wrappers. It will be easy to submit tools to Intergalactic Utilities Commission IUC , if you followed the above standards. emoji cheatsheet reference used in this documentation.","title":"Guidelines"},{"location":"planemo/#install","text":"Follow the instructions in the documentation to install on your machine.","title":"Install"},{"location":"planemo/#using-planemo","text":"Three step process that every tool need to go through : planemo lint your_tool.xml planemo test your_tool.xml --galaxy_root=[path to your local galaxy instance] --test_data [path to the directory containing your testdata] planemo serve your_tool.xml --galaxy_root=[path to your local galaxy instance] If you don't want to use (or) don't have your local galaxy for testing, Planemo will download spin-up a standalone galaxy to test your wrappers. make sure you installed Planemo within a python environment as instructed in installation documentation for Planemo . Each of the above commands supports --help option. use it for the entire list of supported options. If --no-xsd option is used for linting the tool wrapper, Planemo finds errors and warnings within your wrappers by ignoring the validation of correct XML schema. (read the documentation for more info, you might generally want to avoid using this option) planemo test expects test data to be available in a folder named as test-data . you need to write tool specific tests and also provide test data for testing the wrapper, before running the command.","title":"Using Planemo"},{"location":"planemo/#writing-functional-tests-for-tool-wrappers","text":"References to writing proper tests Basic Tests Using tests /tests XML tag Advanced Tests Using reStructuredText to format help section within the tool wrapper - documentation Use the online restructedText editor to write and format your content under help /help section for the wrapper. planemo creates a tool_test_output.html file to showing the errors and debugging information if tools fail otherwise, shows the test output. planemo serve lets you visualize the tools wrapper in GALAXY instance once it passes the tests. Flake8 pycodestyle for python tools Python scripts must conform to pep8 standards and also pass flake8 , so that your tools pass the travisCI tests done by IUC . pycodestyle is the utility to test your python scripts for pep8 coding standards pycodestyle --show-pep8 --show-source --ignore=E501 your_tool .py For the case of submitting and testing the pep8 conformations, the IUC ignores some of the minor errors , such as E501, E201, E202. Refer the below issue on IUC, on what was actually proposed. tools-iuc/issue/467 I would install flake8 and flake8-import-order to do a simple test to remove any errors that pop up after pep8. Below is the command to test the flake8 on the scripts within the current directory. # installing flake8 and flake8-import-order pip install flake8 flake8-import-order # Testing for flake8 errors flake8 --ignore=E201,E202,E501 --count . flake8 documentation : here flake8-import-order : here pycodestyle documentation: here","title":"Writing functional tests for Tool Wrappers"}]}